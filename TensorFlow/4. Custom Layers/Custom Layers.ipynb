{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b4e6e7",
   "metadata": {},
   "source": [
    "# <font color='#154360'> <b> <center> CUSTOM LAYERS </center> </b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6ed612",
   "metadata": {},
   "source": [
    "## <font color='blue'> Table of Contents </font>\n",
    "\n",
    "1. [Introduction](#1)\n",
    "2. [Setup](#2)  \n",
    "3. [Custom Lambda Layers](#3)\n",
    "4. [Custom Layers with Weights](#4)\n",
    "5. [Activation in custom layers](#5) <br>\n",
    "6. [References](#references)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71b8728",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## <font color='blue'> 1. Introduction </font> \n",
    "\n",
    "This notebook demonstrates how to create custom layers in TensorFlow. We’ll cover key concepts, including defining the call and build methods, handling trainable parameters, and integrating custom layers into models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfff047a",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## <font color='blue'> 2. Setup </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b91ed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d62b12d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "SEED = 42\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c42630",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## <font color='blue'> 3. Custom Lambda Layers </font> \n",
    "\n",
    "The Lambda layer in TensorFlow allows you to define simple custom operations without creating a full Layer subclass. It's useful for applying custom functions within a model while keeping the code concise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd418fb",
   "metadata": {},
   "source": [
    "### Example 1: Absolute value\n",
    "\n",
    "In the first example, we will see a lambda layer that applies the absolute value to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14fe470c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "Input: [[-1.  2. -3.]]\n",
      "Output without Lambda layer: [[-0.07905126  0.7423949  -0.42413902]]\n",
      "Output with Lambda layer: [[0.813118  1.1552937 1.9624093]]\n"
     ]
    }
   ],
   "source": [
    "# Define a simple model without Lambda layer\n",
    "model_without_lambda = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(3,)),  # Input shape with 3 features\n",
    "    tf.keras.layers.Dense(3)  # Simple dense layer\n",
    "])\n",
    "\n",
    "# Define a model with Lambda layer applied to output\n",
    "model_with_lambda = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(3,)),\n",
    "    tf.keras.layers.Dense(3),\n",
    "    tf.keras.layers.Lambda(lambda x: tf.abs(x)) # Lambda layer!!!\n",
    "])\n",
    "\n",
    "# Create test input\n",
    "test_input = np.array([[-1.0, 2.0, -3.0]])\n",
    "\n",
    "# Run inference\n",
    "output_without_lambda = model_without_lambda.predict(test_input)\n",
    "output_with_lambda = model_with_lambda.predict(test_input)\n",
    "\n",
    "print(\"Input:\", test_input)\n",
    "print(\"Output without Lambda layer:\", output_without_lambda)\n",
    "print(\"Output with Lambda layer:\", output_with_lambda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3895ac43",
   "metadata": {},
   "source": [
    "To clarify, we can use a custom function to print the values before and after applying the absolute value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65e64041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[0.87977674 0.01321019 0.50682292]]\n",
      "Input to Lambda layer: [[-0.588795424 -0.215181828 1.25408578]]\n",
      "Output from Lambda layer: [[0.588795424 0.215181828 1.25408578]]\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Final output: [[0.5887954  0.21518183 1.2540858 ]]\n"
     ]
    }
   ],
   "source": [
    "# Define a custom Lambda layer that prints the input and output\n",
    "class PrintInputOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, lambda_func):\n",
    "        super(PrintInputOutput, self).__init__()\n",
    "        self.lambda_func = lambda_func\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Print input to Lambda layer (symbolic tensor)\n",
    "        tf.print(\"Input to Lambda layer:\", inputs)\n",
    "        output = self.lambda_func(inputs)\n",
    "        # Print output from Lambda layer (symbolic tensor)\n",
    "        tf.print(\"Output from Lambda layer:\", output)\n",
    "        return output\n",
    "\n",
    "# Define the model\n",
    "model_with_lambda = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(3,)),\n",
    "    tf.keras.layers.Dense(3),\n",
    "    PrintInputOutput(lambda x: tf.abs(x))  # Apply the Lambda operation\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_with_lambda.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Example dummy data for inference\n",
    "X = np.random.random((1, 3))  # 1 sample, 3 features\n",
    "\n",
    "# Perform inference and print the input and output\n",
    "print(\"Input:\", X)\n",
    "output = model_with_lambda.predict(X)\n",
    "print(\"Final output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d219f47",
   "metadata": {},
   "source": [
    "### Example 2: Scaling \n",
    "    \n",
    "Now, we are going to implement a lambda layer that multiplies the output by 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0dcbf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Input: [[-1.  2. -3.]]\n",
      "Output without Lambda layer: [[-0.93204284  3.467924    2.4794106 ]]\n",
      "Output with Lambda layer: [[41.085434 40.95478  10.633764]]\n"
     ]
    }
   ],
   "source": [
    "# Define a simple model without Lambda layer\n",
    "model_without_lambda = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(3,)),  # Input shape with 3 features\n",
    "    tf.keras.layers.Dense(3)  # Simple dense layer\n",
    "])\n",
    "\n",
    "# Define a model with Lambda layer applied to output\n",
    "model_with_lambda = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(3,)),\n",
    "    tf.keras.layers.Dense(3),\n",
    "    tf.keras.layers.Lambda(lambda x: x * 10)\n",
    "])\n",
    "\n",
    "# Create test input\n",
    "test_input = np.array([[-1.0, 2.0, -3.0]])\n",
    "\n",
    "# Run inference\n",
    "output_without_lambda = model_without_lambda.predict(test_input)\n",
    "output_with_lambda = model_with_lambda.predict(test_input)\n",
    "\n",
    "print(\"Input:\", test_input)\n",
    "print(\"Output without Lambda layer:\", output_without_lambda)\n",
    "print(\"Output with Lambda layer:\", output_with_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac708d1",
   "metadata": {},
   "source": [
    "### Example 3: Custom function \n",
    "    \n",
    "Finally, we will implement a lambda layer that applies a leaky ReLU. In this case, we will also train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9195acff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 2.4635 - accuracy: 0.0990\n",
      "Epoch 2/5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 2.3184 - accuracy: 0.1430\n",
      "Epoch 3/5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 2.2587 - accuracy: 0.1300\n",
      "Epoch 4/5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 2.2038 - accuracy: 0.1960\n",
      "Epoch 5/5\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 2.1239 - accuracy: 0.2340\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Output with Lambda layer: [[0.04984679 0.07913906 0.06390966 0.05209231 0.19305727 0.07833236\n",
      "  0.09449093 0.1550185  0.17974283 0.05437021]]\n"
     ]
    }
   ],
   "source": [
    "# Define a custom Leaky ReLU function\n",
    "def my_leaky_relu(x):\n",
    "    return K.maximum(0.1 * x, x)\n",
    "\n",
    "\n",
    "# Define a model with Lambda layer applying Leaky ReLU\n",
    "model_with_lambda = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.Lambda(my_leaky_relu),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile models\n",
    "model_with_lambda.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create fake training data\n",
    "x_train = np.random.rand(1000, 28, 28)\n",
    "y_train = np.random.randint(0, 10, 1000)\n",
    "\n",
    "# Train models\n",
    "model_with_lambda.fit(x_train, y_train, epochs=5, batch_size=32, verbose=1)\n",
    "\n",
    "# Create test input\n",
    "test_input = np.random.rand(1, 28, 28)  # Random input\n",
    "\n",
    "# Run inference\n",
    "output_with_lambda = model_with_lambda.predict(test_input)\n",
    "\n",
    "print(\"Output with Lambda layer:\", output_with_lambda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1aaead",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## <font color='blue'> 4. Custom Layers with Weights </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c52b51",
   "metadata": {},
   "source": [
    "We'll see how to create a custom layer that inherits the Layer class. Unlike simple Lambda layers, the custom layer here will contain weights that can be updated during training.\n",
    "\n",
    "To make a custom layer that is trainable, we need to define a class that inherits the Layer base class from Keras. \n",
    "\n",
    "This class requires three functions: \n",
    "\n",
    "- `__init__()` \n",
    "- build()\n",
    "- call()\n",
    "\n",
    "These ensure that our custom layer has a state and computation that can be accessed during training or inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5158b1f9",
   "metadata": {},
   "source": [
    "```\n",
    "class CustomLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, ...):  \n",
    "        super(CustomLayer, self).__init__(...)\n",
    "        # Initialize attributes\n",
    "\n",
    "    def build(self, input_shape):  \n",
    "        # Define layer's weights\n",
    "        self.some_weight = self.add_weight(...)\n",
    "\n",
    "    def call(self, inputs):  \n",
    "        # Define the forward pass\n",
    "        return some_transformation(inputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ceadd8",
   "metadata": {},
   "source": [
    "### Example 1: Custom Dense Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b592ed",
   "metadata": {},
   "source": [
    "In a dense layer the output is computed as:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\sigma(\\mathbf{W} \\mathbf{x} + \\mathbf{b})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- **x** is the input vector (with shape \\(n\\)).\n",
    "- **W** is the weight matrix (with shape m x n), where m is the number of neurons in the layer).\n",
    "- **b** is the bias vector (with shape \\(m\\)).\n",
    "- **sigma** is the activation function (e.g., ReLU, Sigmoid, etc.).\n",
    "- **y** is the output vector (with shape \\(m\\)).\n",
    "\n",
    "The equation represents a linear transformation of the input, followed by the application of an activation function to introduce non-linearity.\n",
    "\n",
    "In the following example, we will not apply the activation function; we will introduce it later.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d33184d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDense(Layer):\n",
    "    def __init__(self, units=32):\n",
    "        '''Initializes the instance attributes'''\n",
    "        super(SimpleDense, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        '''Create the state of the layer (weights)'''\n",
    "        # Initialize the weights\n",
    "        self.w = self.add_weight(name=\"kernel\",\n",
    "                                 shape=(input_shape[-1], self.units),\n",
    "                                 initializer=tf.random_normal_initializer(),\n",
    "                                 trainable=True)\n",
    "\n",
    "        # Initialize the biases\n",
    "        self.b = self.add_weight(name=\"bias\",\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer=tf.zeros_initializer(),\n",
    "                                 trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        '''Defines the computation from inputs to outputs'''\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f896066b",
   "metadata": {},
   "source": [
    "Let's use this layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57367103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'simple_dense/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[0.01637343]], dtype=float32)>, <tf.Variable 'simple_dense/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# declare an instance of the class\n",
    "my_dense = SimpleDense(units=1)\n",
    "\n",
    "# define an input and feed into the layer\n",
    "x = tf.ones((1, 1))\n",
    "y = my_dense(x)\n",
    "\n",
    "# parameters of the base Layer class like `variables` can be used\n",
    "print(my_dense.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4a247a",
   "metadata": {},
   "source": [
    "Now, let's use this layer in a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffd17dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "[[18.98151]]\n"
     ]
    }
   ],
   "source": [
    "# Define dataset (reshape to 2D: (samples, features))\n",
    "xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float).reshape(-1, 1)\n",
    "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float).reshape(-1, 1)\n",
    "\n",
    "# my custom layer\n",
    "my_layer = SimpleDense(units=1)\n",
    "\n",
    "# simple model that uses our custom layer\n",
    "model = tf.keras.Sequential([my_layer])\n",
    "\n",
    "# compile\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "\n",
    "# train\n",
    "model.fit(xs, ys, epochs=500, verbose=0)\n",
    "\n",
    "# perform inference\n",
    "print(model.predict([[10.0]]))  # Reshaped input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959c6975",
   "metadata": {},
   "source": [
    "### Example 2: Quadratic Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7deaca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquareLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom Keras layer that squares the input tensor element-wise.\n",
    "    This layer applies the square function to each element of the input tensor.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    call(inputs):\n",
    "        Computes the element-wise square of the input tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SquareLayer, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Apply element-wise square to the input tensor\n",
    "        return tf.square(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31ffdd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "Input: [[ 1. -2.  3.]]\n",
      "Output: [[1. 4. 9.]]\n"
     ]
    }
   ],
   "source": [
    "# Create a model with the custom layer\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(3,)),  \n",
    "    SquareLayer()\n",
    "])\n",
    "\n",
    "# Test the layer with sample input\n",
    "test_input = np.array([[1.0, -2.0, 3.0]])\n",
    "output = model.predict(test_input)\n",
    "\n",
    "print(\"Input:\", test_input)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5df5213",
   "metadata": {},
   "source": [
    "### Example 3: Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a53a0c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom Keras layer that scales the input tensor by a trainable scalar weight.\n",
    "\n",
    "    This layer multiplies the input tensor by a scalar weight, which is initialized\n",
    "    with a given value (default is 1.0) and is trainable during model training.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    scale: tf.Variable\n",
    "        A trainable scalar that scales the input tensor.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    build(input_shape):\n",
    "        Initializes the trainable scalar weight.\n",
    "    \n",
    "    call(inputs):\n",
    "        Scales the input tensor by the trainable scalar.\n",
    "    \"\"\"\n",
    "    def __init__(self, initial_scale=1.0):\n",
    "        super(ScaleLayer, self).__init__()\n",
    "        self.initial_scale = initial_scale\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Initialize the trainable scalar weight with a constant initializer\n",
    "        self.scale = self.add_weight(\n",
    "            name=\"scale\",\n",
    "            shape=(1,),\n",
    "            initializer=tf.constant_initializer(self.initial_scale),\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Scale the input tensor by the trainable scalar weight\n",
    "        return inputs * self.scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "314033ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "Test Input: [[5.]]\n",
      "Predicted Output: [[15.]]\n",
      "Learned Scale Factor: [3.]\n"
     ]
    }
   ],
   "source": [
    "# Create fake dataset (y = 3 * x)\n",
    "x_train = np.random.rand(1000, 1).astype(np.float32) * 10\n",
    "y_train = 3 * x_train  \n",
    "\n",
    "# Create model with the custom layer\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,)),\n",
    "    ScaleLayer(initial_scale=1.0)  # Trainable scaling factor\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=100, verbose=0)\n",
    "\n",
    "# Test the model\n",
    "test_input = np.array([[5.0]], dtype=np.float32)\n",
    "output = model.predict(test_input)\n",
    "\n",
    "print(\"Test Input:\", test_input)\n",
    "print(\"Predicted Output:\", output)\n",
    "print(\"Learned Scale Factor:\", model.layers[0].get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61ce3ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "Test Input: [[15.]]\n",
      "Predicted Output: [[45.]]\n",
      "Learned Scale Factor: [3.]\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "test_input = np.array([[15.0]], dtype=np.float32)\n",
    "output = model.predict(test_input)\n",
    "\n",
    "print(\"Test Input:\", test_input)\n",
    "print(\"Predicted Output:\", output)\n",
    "print(\"Learned Scale Factor:\", model.layers[0].get_weights()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d36d644",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## <font color='blue'> 5. Activation in custom layers </font> \n",
    "\n",
    "To use the built-in activations in Keras, we can:\n",
    "\n",
    "- Specify an activation parameter in the __init__() method of our custom layer class. \n",
    "\n",
    "- From there, we can initialize it by using the tf.keras.activations.get() method. This takes in a string identifier that corresponds to one of the available activations in Keras. \n",
    "\n",
    "- Next, you can now pass in the forward computation to this activation in the call() method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df2f71",
   "metadata": {},
   "source": [
    "### Example 1: Custom Dense Layer with Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5a67d28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=32, activation=None):\n",
    "        \"\"\"\n",
    "        Custom dense layer with optional activation function.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        units: int\n",
    "            The number of units (neurons) in the dense layer.\n",
    "        activation: str or callable, optional\n",
    "            The activation function to apply after the linear transformation.\n",
    "            If None, no activation function will be applied.\n",
    "        \"\"\"\n",
    "        super(SimpleDense, self).__init__()\n",
    "        self.units = units\n",
    "        # Get the activation function using Keras' utility function.\n",
    "        # This supports string identifiers (e.g., 'relu', 'sigmoid') or custom functions.\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(name=\"kernel\",\n",
    "                                 shape=(input_shape[-1], self.units),\n",
    "                                 initializer=tf.keras.initializers.RandomNormal(stddev=0.1),\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name=\"bias\",\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer=tf.zeros_initializer(),\n",
    "                                 trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Perform the linear transformation (matmul + bias)\n",
    "        output = tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "        # Apply the activation function if specified.  Crucial fix!\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)  # Or: return self.activation(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b91523f",
   "metadata": {},
   "source": [
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "551ec3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fake Data (Linear Relationship) ---\n",
    "x_train = np.array([[1], [2], [3], [4], [5]], dtype=np.float32)  # Input\n",
    "y_train = np.array([[2], [4], [6], [8], [10]], dtype=np.float32) # Output (y = 2x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faa5e03",
   "metadata": {},
   "source": [
    "First, we will not use an activation function (activation = None in SimpleDense):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40e8cbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Model with Custom Layer ---\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,)),  # Input layer\n",
    "    SimpleDense(units=1, activation=None)  # Custom Dense layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ed4191fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "Test Input: [[6.]]\n",
      "Predicted Output: [[11.938638]]\n",
      "Learned Weights: [array([[1.974322]], dtype=float32), array([0.09270614], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# --- Train Model ---\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01), loss='mse')\n",
    "model.fit(x_train, y_train, epochs=500, verbose=0)\n",
    "\n",
    "# --- Test Model ---\n",
    "test_input = np.array([[6]], dtype=np.float32)  # Expected output: 12\n",
    "output = model.predict(test_input)\n",
    "\n",
    "print(\"Test Input:\", test_input)\n",
    "print(\"Predicted Output:\", output)\n",
    "print(\"Learned Weights:\", model.layers[0].get_weights())  # Check trained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9390b14f",
   "metadata": {},
   "source": [
    "Now let's use ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4dd5fe39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "Test Input: [[6.]]\n",
      "Predicted Output: [[11.939157]]\n",
      "Learned Weights: [array([[1.9745392]], dtype=float32), array([0.09192166], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# --- Define Model with Custom Layer ---\n",
    "model_relu = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,)),  # Input layer\n",
    "    SimpleDense(units=1, activation='relu')  # Custom Dense layer with ReLU\n",
    "])\n",
    "\n",
    "# --- Train Model ---\n",
    "model_relu.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01), loss='mse')\n",
    "model_relu.fit(x_train, y_train, epochs=500, verbose=0)\n",
    "\n",
    "# --- Test Model ---\n",
    "test_input = np.array([[6]], dtype=np.float32)  # Expected output: 12\n",
    "output = model_relu.predict(test_input)\n",
    "\n",
    "print(\"Test Input:\", test_input)\n",
    "print(\"Predicted Output:\", output)\n",
    "print(\"Learned Weights:\", model_relu.layers[0].get_weights())  # Check trained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef016688",
   "metadata": {},
   "source": [
    "### Example 2: Quadratic Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f7f6fc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom quadratic layer\n",
    "class SimpleQuadratic(Layer):\n",
    "    def __init__(self, units=32, activation=None):\n",
    "        \"\"\"\n",
    "        Custom quadratic layer that applies a quadratic transformation to the input.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        units: int\n",
    "            The number of units (neurons) in the layer.\n",
    "        activation: str or callable, optional\n",
    "            The activation function to apply after the quadratic transformation.\n",
    "            If None, no activation function will be applied.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        # Get the activation function using Keras' utility function\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        Initialize the weights for the quadratic transformation: \n",
    "        - a: coefficient for the squared input.\n",
    "        - b: coefficient for the linear input.\n",
    "        - c: constant term.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_shape: tuple\n",
    "            The shape of the input tensor. We use the last dimension for weight shapes.\n",
    "        \"\"\"\n",
    "        # Initialize the weights a, b, and c with random normal and zeros\n",
    "        a_init = tf.random_normal_initializer()\n",
    "        b_init = tf.random_normal_initializer()\n",
    "        c_init = tf.zeros_initializer()\n",
    "\n",
    "        # Define the trainable weights\n",
    "        self.a = self.add_weight(shape=(input_shape[-1], self.units), initializer=a_init, trainable=True, name=\"a\")\n",
    "        self.b = self.add_weight(shape=(input_shape[-1], self.units), initializer=b_init, trainable=True, name=\"b\")\n",
    "        self.c = self.add_weight(shape=(self.units,), initializer=c_init, trainable=True, name=\"c\")\n",
    "   \n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Apply the quadratic transformation: \n",
    "        - First, square the inputs.\n",
    "        - Then compute the quadratic terms (ax² + bx + c).\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        inputs: Tensor\n",
    "            The input tensor to be transformed.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        output: Tensor\n",
    "            The transformed tensor, optionally passed through an activation function.\n",
    "        \"\"\"\n",
    "        # Square the input tensor element-wise\n",
    "        x_squared = tf.math.square(inputs)\n",
    "        \n",
    "        # Compute the quadratic transformation terms\n",
    "        x_squared_times_a = tf.linalg.matmul(x_squared, self.a)\n",
    "        x_times_b = tf.linalg.matmul(inputs, self.b)\n",
    "        \n",
    "        # Combine the terms (ax² + bx + c)\n",
    "        output = x_squared_times_a + x_times_b + self.c\n",
    "        \n",
    "        # Apply activation function if specified\n",
    "        return self.activation(output) if self.activation else output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bd814f",
   "metadata": {},
   "source": [
    "Let's try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "da53dca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 20:00:21.262797: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2717 - accuracy: 0.9186\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1341 - accuracy: 0.9602\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1038 - accuracy: 0.9685\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0834 - accuracy: 0.9737\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0730 - accuracy: 0.9774\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.0811 - accuracy: 0.9748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08106440305709839, 0.9747999906539917]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  SimpleQuadratic(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e51a11",
   "metadata": {},
   "source": [
    "Let's change the activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8e4c440f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 20:02:08.922326: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2914 - accuracy: 0.9134\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1691 - accuracy: 0.9501\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1392 - accuracy: 0.9571\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1233 - accuracy: 0.9617\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1112 - accuracy: 0.9656\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.1019 - accuracy: 0.9701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10189716517925262, 0.9700999855995178]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  SimpleQuadratic(128, activation='leaky_relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c8dabf",
   "metadata": {},
   "source": [
    "Finally, let's not use the quadratic layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0e43f97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 20:01:05.475102: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.5050 - accuracy: 0.8630\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3431 - accuracy: 0.9021\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3239 - accuracy: 0.9081\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3163 - accuracy: 0.9096\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3129 - accuracy: 0.9108\n",
      "313/313 [==============================] - 0s 957us/step - loss: 0.2702 - accuracy: 0.9243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.27015531063079834, 0.9243000149726868]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf23562",
   "metadata": {},
   "source": [
    "In this example, we obtained the best results using the quadratic layer with ReLU as the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7be397",
   "metadata": {},
   "source": [
    "<a name=\"references\"></a>\n",
    "## <font color='blue'> 6. References </font> \n",
    "\n",
    "[TensorFlow Advanced Techniques Specialization](https://www.coursera.org/specializations/tensorflow-advanced-techniques)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
