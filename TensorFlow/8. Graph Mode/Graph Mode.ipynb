{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "665eecbb",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center; color: #1a5276;\">Graph Mode</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85920d79",
   "metadata": {},
   "source": [
    "## <font color='blue'> Table of Contents </font>\n",
    "\n",
    "1. [Introduction](#1)\n",
    "2. [Setup](#2)  \n",
    "3. [Simple examples](#3)\n",
    "4. [Print behavior](#4)\n",
    "5. [Training](#5)\n",
    "6. [References](#references)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012f67dc",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## <font color='blue'> 1. Introduction </font>\n",
    "\n",
    "Graph Mode is a powerful feature where your computations are first defined as a computation graph before being executed. This mode is used primarily in TensorFlow 1.x and still available in TensorFlow 2.x through @tf.function.\n",
    "\n",
    "Key benefits:\n",
    "\n",
    "- Performance Optimization\n",
    "    - Since the graph is static, TensorFlow can apply various optimizations like constant folding, kernel fusion, etc., improving execution speed.\n",
    "\n",
    "- Deployment Efficiency \n",
    "    - Graphs can be serialized, exported, and run on different platforms (e.g., mobile devices, TPU, etc.).\n",
    "\n",
    "- Parallel Execution\n",
    "    - The static graph enables parallel computation across devices efficiently.\n",
    "\n",
    "- Error Checking\n",
    "    - Graphs are analyzed before execution, allowing TensorFlow to catch potential issues early.\n",
    "\n",
    "\n",
    "For debugging or quick prototyping, Eager Execution is generally more intuitive, but Graph Mode excels in production scenarios requiring efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8109c950",
   "metadata": {},
   "source": [
    "Graph Mode: When you use @tf.function, TensorFlow traces the function once (using a sample input or the first value you provide), creating a static computation graph. This graph includes all operations needed for forward and backward passes (like matrix multiplications, activations, gradients, etc.). It doesn’t dynamically change once traced — meaning the structure is fixed, but the values computed during the forward and backward passes will depend on the input data and current model parameters.\n",
    "\n",
    "Eager Execution: When not using @tf.function, TensorFlow operates in eager execution mode, which means that operations are evaluated immediately as you run the code, and the graph is built dynamically as the operations are executed. This allows for more flexibility but is slower than graph execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05159d6",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## <font color='blue'> 2. Setup </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8cd3a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import time  \n",
    "import numpy as np  \n",
    "import tensorflow as tf  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "523ffac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "np.random.seed(seed)                    \n",
    "tf.random.set_seed(seed)                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a428f2e4",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## <font color='blue'> 3. Simple Examples </font>\n",
    "\n",
    "### Adding tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "241e9963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(8, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function  # Converts to Graph Mode\n",
    "def add_tensors(x, y):\n",
    "    return x + y\n",
    "\n",
    "# Example usage\n",
    "a = tf.constant(3)\n",
    "b = tf.constant(5)\n",
    "print(add_tensors(a, b))  # Output: 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e21bd8e",
   "metadata": {},
   "source": [
    "Let's see the generted code:\n",
    "\n",
    "- tf.autograph.to_code(): This function takes a Python function as input and returns the generated TensorFlow graph code (in Python syntax) that TensorFlow uses internally to optimize execution.\n",
    "\n",
    "\n",
    "- add_tensors.python_function: Since add_tensors is a @tf.function-decorated function, .python_function accesses the original Python implementation before TensorFlow compiles it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e4fa71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def tf__add_tensors(x, y):\n",
      "    with ag__.FunctionScope('add_tensors', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = ag__.ld(x) + ag__.ld(y)\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See what the generated code looks like\n",
    "print(tf.autograph.to_code(add_tensors.python_function))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270427f7",
   "metadata": {},
   "source": [
    "### Visualizing the computation graph\n",
    "\n",
    "We can use TensorBoard to visualize the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64cdb142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your function with @tf.function to enable graph mode\n",
    "@tf.function\n",
    "def add_tensors(x, y):\n",
    "    return x + y\n",
    "\n",
    "# Create a directory for TensorBoard logs\n",
    "log_dir = \"logs/graph\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Create a SummaryWriter to log the graph\n",
    "writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "# Log the graph to the writer\n",
    "with writer.as_default():\n",
    "    # Log the graph using tf.summary.trace_on() and tf.summary.trace_export()\n",
    "    tf.summary.trace_on(graph=True)\n",
    "    a = tf.constant(3)\n",
    "    b = tf.constant(5)\n",
    "    add_tensors(a, b)\n",
    "    tf.summary.trace_export(name=\"model_trace\", step=0, profiler_outdir=log_dir)\n",
    "\n",
    "# Start TensorBoard (in the terminal or an IDE with TensorBoard support)\n",
    "# Run the following in your terminal:\n",
    "# tensorboard --logdir=logs/graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5bf44f",
   "metadata": {},
   "source": [
    "If we open http://localhost:6006 in our browser we can view the graph under the Graph tab in TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dfbf1b",
   "metadata": {},
   "source": [
    "<img src=\"images/ComputationGraph.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8750d313",
   "metadata": {},
   "source": [
    "### Control flows\n",
    "\n",
    "Control flow statements which are very intuitive to write in eager mode can look very complex in graph mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14bd5954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def tf__f(x):\n",
      "    with ag__.FunctionScope('f', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "\n",
      "        def get_state():\n",
      "            return (x,)\n",
      "\n",
      "        def set_state(vars_):\n",
      "            nonlocal x\n",
      "            (x,) = vars_\n",
      "\n",
      "        def if_body():\n",
      "            nonlocal x\n",
      "            x = ag__.ld(x) * ag__.ld(x)\n",
      "\n",
      "        def else_body():\n",
      "            nonlocal x\n",
      "            pass\n",
      "        ag__.if_stmt(ag__.ld(x) > 12, if_body, else_body, get_state, set_state, ('x',), 1)\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = ag__.ld(x)\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# simple function that returns the square if the input is greater than zero\n",
    "@tf.function\n",
    "def f(x):\n",
    "    if x>12:\n",
    "        x = x * x\n",
    "    return x\n",
    "\n",
    "print(tf.autograph.to_code(f.python_function))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f05f53",
   "metadata": {},
   "source": [
    "### Common errors\n",
    "\n",
    "The following code will raise an exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "add52df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: ValueError('tf.function only supports singleton tf.Variables created on the first call. Make sure the tf.Variable is only created once or created outside tf.function. See https://www.tensorflow.org/guide/function#creating_tfvariables for more information.')\n",
      "Tip: Define `weights` and `biases` outside the `@tf.function` block to avoid this issue.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "@tf.function\n",
    "def simple_model(x):\n",
    "    try:\n",
    "        weights = tf.Variable([[0.1, 0.2], [0.3, 0.4]])\n",
    "        biases = tf.Variable([0.5, 0.6])\n",
    "        output = tf.matmul(x, weights) + biases\n",
    "        return tf.nn.relu(output)\n",
    "    except ValueError as e:\n",
    "        tf.print(\"Error:\", e)\n",
    "        tf.print(\"Tip: Define `weights` and `biases` outside the `@tf.function` block to avoid this issue.\")\n",
    "\n",
    "# Example input data\n",
    "x = tf.constant([[1.0, 2.0]])\n",
    "\n",
    "# Forward pass\n",
    "simple_model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5880ee96",
   "metadata": {},
   "source": [
    "The error occurs because TensorFlow's @tf.function decorator requires that tf.Variable objects are created outside the decorated function or only once during the first call. When you define weights and biases inside the @tf.function-decorated function, TensorFlow tries to recreate these variables every time the function is called, which is not allowed.\n",
    "\n",
    "To fix this, you need to create the tf.Variable objects once, outside of the function, and then use them inside the @tf.function function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a6979c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1.2 1.6]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Define the model with tf.function\n",
    "@tf.function\n",
    "def simple_model(x, weights, biases):\n",
    "    output = tf.matmul(x, weights) + biases\n",
    "    return tf.nn.relu(output)\n",
    "\n",
    "# Create variables outside the tf.function\n",
    "weights = tf.Variable([[0.1, 0.2], [0.3, 0.4]])\n",
    "biases = tf.Variable([0.5, 0.6])\n",
    "\n",
    "# Example input data\n",
    "x = tf.constant([[1.0, 2.0]])\n",
    "\n",
    "# Forward pass\n",
    "print(simple_model(x, weights, biases))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43c1a40",
   "metadata": {},
   "source": [
    "In the previous code:\n",
    "    \n",
    "- The structure of the graph is defined once when you call simple_model(x) for the first time.\n",
    "\n",
    "- Values like weights, and biases will change as you pass new data during training.\n",
    "\n",
    "- The graph stays the same, but the values for activations and gradients will change depending on the data and current model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7106e2d5",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## <font color='blue'> 4. Print behavior </font>\n",
    "\n",
    "Consider the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7177e27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traced with 2\n",
      "Traced with 2\n",
      "Traced with 2\n",
      "Traced with 2\n",
      "Traced with 2\n"
     ]
    }
   ],
   "source": [
    "## print\n",
    "def f(x):\n",
    "    print(\"Traced with\", x)\n",
    "\n",
    "for i in range(5):\n",
    "    f(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e350e725",
   "metadata": {},
   "source": [
    "The previous code shows the normal behaviour of the print function. Now, consider the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6708e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traced with 2\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def f(x):\n",
    "    print(\"Traced with\", x)\n",
    "\n",
    "for i in range(5):\n",
    "    f(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a95893",
   "metadata": {},
   "source": [
    "Even though the loop runs 5 times, the print statement only executes once. To solve this, we need to use tf.print() instead of print()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f1d0f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traced with 2\n",
      "Executed with 2\n",
      "Executed with 2\n",
      "Executed with 2\n",
      "Executed with 2\n",
      "Executed with 2\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def f(x):\n",
    "    print(\"Traced with\", x)\n",
    "    # added tf.print\n",
    "    tf.print(\"Executed with\", x)\n",
    "\n",
    "for i in range(5):\n",
    "    f(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633a8e4d",
   "metadata": {},
   "source": [
    "**Key takeaway:** tf.print is graph aware and will run as expected in loops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f02e5d0",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## <font color='blue'> 5. Training </font>\n",
    "\n",
    "We will train a simple model with and without graph mode and compare the execution time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0288a692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with no graph mode took 13.1743 seconds.\n",
      "Training with graph mode took 1.0698 seconds.\n",
      "Speedup from graph mode: 12.31x\n"
     ]
    }
   ],
   "source": [
    "# Define a simple model for binary classification\n",
    "class SimpleModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        return self.dense2(x)\n",
    "\n",
    "# Loss and optimization functions\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Generate random data for training (1000 samples, 20 features)\n",
    "X_train = np.random.randn(1000, 20).astype(np.float32)\n",
    "y_train = np.random.randint(0, 2, size=(1000, 1)).astype(np.float32)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = SimpleModel()\n",
    "\n",
    "# Training step without @tf.function\n",
    "def train_step_no_graph(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(y, logits)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Training step with @tf.function (graph mode)\n",
    "@tf.function\n",
    "def train_step_with_graph(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(y, logits)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Helper function to train for one epoch\n",
    "def train_epoch(train_data, train_labels, train_step_fn):\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in zip(train_data, train_labels):\n",
    "        # Ensure x_batch has the correct shape (batch_size, num_features)\n",
    "        if x_batch.ndim == 1:\n",
    "            x_batch = np.expand_dims(x_batch, axis=0)\n",
    "        # Ensure y_batch has the correct shape (batch_size, 1)\n",
    "        if y_batch.ndim == 1:\n",
    "            y_batch = np.expand_dims(y_batch, axis=0)\n",
    "        loss = train_step_fn(x_batch, y_batch)\n",
    "        total_loss += loss\n",
    "    return total_loss / len(train_data)\n",
    "\n",
    "# Fix the input shape by ensuring X_train has a batch dimension\n",
    "if X_train.ndim == 1:\n",
    "    X_train = X_train.reshape(-1, 20)  # Ensures input is (batch_size, 20)\n",
    "\n",
    "# Train with no graph mode (normal Python function)\n",
    "start_time = time.time()\n",
    "train_epoch(X_train, y_train, train_step_no_graph)\n",
    "end_time = time.time()\n",
    "no_graph_duration = end_time - start_time\n",
    "print(f\"Training with no graph mode took {no_graph_duration:.4f} seconds.\")\n",
    "\n",
    "# Train with graph mode (tf.function)\n",
    "start_time = time.time()\n",
    "train_epoch(X_train, y_train, train_step_with_graph)\n",
    "end_time = time.time()\n",
    "graph_duration = end_time - start_time\n",
    "print(f\"Training with graph mode took {graph_duration:.4f} seconds.\")\n",
    "\n",
    "# Compare the results\n",
    "print(f\"Speedup from graph mode: {no_graph_duration / graph_duration:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fbff0c",
   "metadata": {},
   "source": [
    "The results show a significant performance boost using graph mode. The training time decreased from 13.17s to 1.07s, achieving a 12.31x speedup. This highlights TensorFlow's efficiency in optimizing code execution with graph mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6ba109",
   "metadata": {},
   "source": [
    "<a name=\"6\"></a>\n",
    "## <font color='blue'> 6. Summary </font>\n",
    "\n",
    "**Summary of Key Benefits:**\n",
    "\n",
    "- Performance Boost: Faster execution through optimization, better memory management, and reduced Python overhead.\n",
    "\n",
    "- Hardware Optimization: Efficient execution on GPUs/TPUs with hardware-specific optimizations.\n",
    "\n",
    "- Portability: Easier deployment and execution across different platforms and hardware.\n",
    "\n",
    "- Reduced Python Overhead: Compiles operations into a static graph, eliminating the need for repeated function calls in Python.\n",
    "\n",
    "- Efficient Backpropagation: Optimized gradient computation, leading to faster training.\n",
    "\n",
    "By using @tf.function, you're enabling TensorFlow to optimize execution for performance, which is especially important for complex models and large datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a276f0",
   "metadata": {},
   "source": [
    "<a name=\"references\"></a>\n",
    "## <font color='blue'> References </font>\n",
    "\n",
    "[TF Advanced Techniques](https://www.coursera.org/specializations/tensorflow-advanced-techniques)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
