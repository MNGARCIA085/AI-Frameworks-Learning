{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "041aa38d",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center; color: #1a5276;\">Custom Layers</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db137d7",
   "metadata": {},
   "source": [
    "## <font color='blue'>  Table of Contents </font>\n",
    "\n",
    "1. [Introduction](#1)\n",
    "2. [Setup](#2)\n",
    "3. [Training](#3) <br>\n",
    "    3.1. [Example 1](#3.1) <br>\n",
    "    3.2. [Example 2](#3.2) <br>\n",
    "4. [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f691910",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## <font color='blue'> 1. Introduction </font>\n",
    "\n",
    "This notebook demonstrates how to build custom layers in PyTorch to extend model flexibility and improve performance. It covers:\n",
    "\n",
    "- Implementing custom layers using nn.Module.\n",
    "\n",
    "- Adding parameters and defining forward logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac2778a",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## <font color='blue'> 2. Setup </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcbfde64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f80728b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdf54bc19f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c67a481",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## <font color='blue'> 3. Examples </font>\n",
    "\n",
    "<a name=\"3.1\"></a>\n",
    "### <font color='#1f618d'> 3.1. Example 1 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed15fed",
   "metadata": {},
   "source": [
    "### Defining the custom layer\n",
    "\n",
    "Here's a simple example of a custom PyTorch layer that performs a weighted sum of inputs followed by a non-linear activation:\n",
    "\n",
    "<img src=\"images/CustomLinear.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca8baced",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom PyTorch layer that applies a linear transformation \n",
    "    followed by a ReLU activation.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Number of input features.\n",
    "        output_dim (int): Number of output features.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        \n",
    "        # Learnable weight parameter for the linear transformation\n",
    "        self.weight = nn.Parameter(torch.randn(input_dim, output_dim))\n",
    "        \n",
    "        # Learnable bias parameter initialized to zero\n",
    "        self.bias = nn.Parameter(torch.zeros(output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the custom layer.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, output_dim) \n",
    "                          after applying the linear transformation and ReLU.\n",
    "        \"\"\"\n",
    "        x = torch.matmul(x, self.weight) + self.bias  # Linear transformation\n",
    "        return F.relu(x)  # ReLU activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01757e6c",
   "metadata": {},
   "source": [
    "This custom layer applies a linear transformation followed by a ReLU activation. Using nn.Parameter allows the layer's weights to be trainable. You can integrate this layer like any other PyTorch module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bd8d58",
   "metadata": {},
   "source": [
    "### Using the custom layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "786967bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple neural network that combines a custom layer with a standard linear layer.\n",
    "\n",
    "    The first layer is a custom layer (`CustomLayer`) that applies a linear \n",
    "    transformation followed by ReLU. The second layer is a standard PyTorch \n",
    "    `nn.Linear` layer that outputs a single value, passed through a sigmoid \n",
    "    activation for binary classification.\n",
    "\n",
    "    This demonstrates how to integrate custom layers into a PyTorch model.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        \n",
    "        # Custom layer: Transforms input features from 10 to 20 dimensions\n",
    "        self.layer1 = CustomLayer(10, 20)\n",
    "        \n",
    "        # Standard linear layer: Transforms 20 features to 1 output\n",
    "        self.layer2 = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, 10).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, 1) with values \n",
    "                          in the range [0, 1] (after sigmoid activation).\n",
    "        \"\"\"\n",
    "        x = self.layer1(x)  # Applying the custom layer\n",
    "        return torch.sigmoid(self.layer2(x))  # Final output with sigmoid activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c89bf7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0847],\n",
      "        [0.1793],\n",
      "        [0.0875],\n",
      "        [0.2268],\n",
      "        [0.1330]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "x = torch.randn(5, 10)  # Batch size 5, input size 10\n",
    "model = SimpleModel()\n",
    "output = model(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af84be39",
   "metadata": {},
   "source": [
    "<a name=\"3.2\"></a>\n",
    "### <font color='#1f618d'> 3.2. Example 2 </font>\n",
    "\n",
    "This updated example includes batch normalization and dropout for improved training stability and regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82995d34",
   "metadata": {},
   "source": [
    "### Defining the custom layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dbec3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom PyTorch layer that applies a linear transformation,\n",
    "    batch normalization, ReLU activation, and dropout.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Number of input features.\n",
    "        output_dim (int): Number of output features.\n",
    "        dropout_rate (float): Dropout rate for regularization (default is 0.3).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, dropout_rate=0.3):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        \n",
    "        # Learnable weight parameter for the linear transformation\n",
    "        self.weight = nn.Parameter(torch.randn(input_dim, output_dim))\n",
    "        \n",
    "        # Learnable bias parameter initialized to zero\n",
    "        self.bias = nn.Parameter(torch.zeros(output_dim))\n",
    "        \n",
    "        # Batch normalization to stabilize training\n",
    "        self.bn = nn.BatchNorm1d(output_dim)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the custom layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, output_dim) \n",
    "                          after applying linear transformation, batch normalization, \n",
    "                          ReLU activation, and dropout.\n",
    "        \"\"\"\n",
    "        x = torch.matmul(x, self.weight) + self.bias  # Linear transformation\n",
    "        x = self.bn(x)  # Batch normalization\n",
    "        x = F.relu(x)   # ReLU activation\n",
    "        return self.dropout(x)  # Dropout for regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aaf0f1",
   "metadata": {},
   "source": [
    "### Using the custom layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f5543aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "class SimpleModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple neural network that combines a custom layer with a standard linear layer.\n",
    "\n",
    "    The first layer is a custom layer (`CustomLayer`) that applies a linear \n",
    "    transformation, batch normalization, ReLU activation, and dropout. \n",
    "    The second layer is a standard PyTorch `nn.Linear` layer that outputs a \n",
    "    single value, passed through a sigmoid activation for binary classification.\n",
    "\n",
    "    This demonstrates how to integrate a custom layer into a PyTorch model.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        \n",
    "        # Custom layer: Transforms input features from 10 to 20 dimensions\n",
    "        self.layer1 = CustomLayer(10, 20)\n",
    "        \n",
    "        # Standard linear layer: Transforms 20 features to 1 output\n",
    "        self.layer2 = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, 10).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, 1) with values \n",
    "                          in the range [0, 1] (after sigmoid activation).\n",
    "        \"\"\"\n",
    "        x = self.layer1(x)  # Applying the custom layer\n",
    "        return torch.sigmoid(self.layer2(x))  # Final output with sigmoid activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b57406f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2799],\n",
      "        [0.4136],\n",
      "        [0.3648],\n",
      "        [0.5205],\n",
      "        [0.3727]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "x = torch.randn(5, 10)  # Batch size 5, input size 10\n",
    "model = SimpleModel()\n",
    "output = model(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83ca507",
   "metadata": {},
   "source": [
    "<a name=\"references\"></a>\n",
    "## <font color='blue'> References </font>\n",
    "\n",
    "[PyTorch Documentation](https://pytorch.org/docs/stable/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
