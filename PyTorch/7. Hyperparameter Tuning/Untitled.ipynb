{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66811a2f-5d6e-4141-b370-147158e0915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d761e5-2422-450b-94bc-1a1a29379cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from ray import tune\n",
    "from ray.air import session\n",
    "from ray.tune.schedulers import ASHAScheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e60dcf-3397-483d-90e6-f88791adf4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    np.random.seed(0)\n",
    "    X = np.random.rand(1000, 5)\n",
    "    y = 3 * X[:, 0] + 2 * X[:, 1] + np.random.randn(1000) * 0.1\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    return (\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).unsqueeze(1),\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "        torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3671326-c5f7-4ce8-af05-b501eb75dc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_data():\n",
    "    np.random.seed(0)\n",
    "    X = np.random.rand(1000, 5)  # 1000 samples, 5 features\n",
    "    y = 3 * X[:, 0] + 2 * X[:, 1] + np.random.randn(1000) * 0.1  # linear with noise\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    # Convert to torch tensors; regression targets as floats with shape (N,1)\n",
    "    return (\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).unsqueeze(1),\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "        torch.tensor(y_val, dtype=torch.float32).unsqueeze(1),\n",
    "    )\n",
    "\n",
    "def plot_data(X_train, y_train):\n",
    "    # Move to numpy for plotting\n",
    "    X = X_train.numpy()\n",
    "    y = y_train.squeeze(1).numpy()\n",
    "\n",
    "    print(f\"Training set size: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "    print(f\"Validation set size: {len(y)} samples\")\n",
    "    print(f\"Target y mean: {y.mean():.4f}, std: {y.std():.4f}\")\n",
    "\n",
    "    # Scatter: y vs first feature\n",
    "    plt.figure()\n",
    "    plt.scatter(X[:, 0], y, s=10)\n",
    "    plt.xlabel(\"Feature X[:,0]\")\n",
    "    plt.ylabel(\"Target y\")\n",
    "    plt.title(\"y vs X[:,0]\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Scatter: y vs second feature\n",
    "    plt.figure()\n",
    "    plt.scatter(X[:, 1], y, s=10)\n",
    "    plt.xlabel(\"Feature X[:,1]\")\n",
    "    plt.ylabel(\"Target y\")\n",
    "    plt.title(\"y vs X[:,1]\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Histogram of y\n",
    "    plt.figure()\n",
    "    plt.hist(y, bins=30)\n",
    "    plt.xlabel(\"Target y\")\n",
    "    plt.title(\"Distribution of y (training)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_train, y_train, X_val, y_val = get_data()\n",
    "    plot_data(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ebd569-9756-4720-9268-3580db7b0e21",
   "metadata": {},
   "source": [
    "Dataset sizes:\n",
    "\n",
    "Training: 800 samples × 5 features\n",
    "\n",
    "Validation: 200 samples × 5 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b47300a-375e-48ce-b916-105c1190fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f5ff99-55a7-43be-9e3d-743044cde04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77ad6f0-5082-4282-85ae-79823d8b625b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d490776c-e949-40c8-898c-e6070d60e296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e485a0-7bcd-41da-b930-4ec8ca16c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27511911-4429-467e-9c25-32a5fe3d07a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f6beab-9005-49e1-a0d6-e0e2798ad382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from ray.air import session\n",
    "#from ray.air.checkpoint import Checkpoint\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        preds = model(xb)  # (N, 1)\n",
    "        loss = criterion(preds, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        count += xb.size(0)\n",
    "\n",
    "    avg_loss = total_loss / count\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def r2_score(preds, targets):\n",
    "    ss_res = torch.sum((targets - preds) ** 2)\n",
    "    ss_tot = torch.sum((targets - torch.mean(targets)) ** 2)\n",
    "    return 1 - ss_res / ss_tot\n",
    "\n",
    "\n",
    "def validate(model, X_val, y_val, criterion, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_val = X_val.to(device)\n",
    "        y_val = y_val.to(device)\n",
    "\n",
    "        preds = model(X_val)\n",
    "        val_loss = criterion(preds, y_val).item()\n",
    "        val_r2 = r2_score(preds, y_val).item()\n",
    "    return val_loss, val_r2\n",
    "\n",
    "\n",
    "def train_model(config):\n",
    "    X_train, y_train, X_val, y_val = get_data()  # your regression data\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = SimpleRegressor(input_dim=X_train.shape[1], hidden_dim=config[\"hidden_dim\"])\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    for epoch in range(int(config[\"max_num_epochs\"])):\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_r2 = validate(model, X_val, y_val, criterion, device)\n",
    "\n",
    "        # Save checkpoint and report to Ray AIR session\n",
    "        \"\"\"\n",
    "        with tempfile.TemporaryDirectory() as td:\n",
    "            path = os.path.join(td, \"model.pt\")\n",
    "            torch.save(model.state_dict(), path)\n",
    "            ckpt = Checkpoint.from_directory(td)\n",
    "\n",
    "            session.report(\n",
    "                {\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"val_r2\": val_r2,\n",
    "                    \"epoch\": epoch,\n",
    "                },\n",
    "                checkpoint=ckpt,\n",
    "            )\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e5c71c-51a4-46e9-b6ff-4a59e5674287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb00d74-2e91-462f-8d52-a231269b7779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6581e8-163e-4571-aeef-77d0631beecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conservative sizes: limit concurrency and shrink object store\n",
    "ray.init(num_cpus=4, object_store_memory=2 * 1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509524d1-c1e4-4ee4-83d9-254bd61510b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ray\n",
    "\n",
    "#ray.init(object_store_memory=500 * 1024 * 1024)  # 500 MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7338f22-f0cb-4a1b-a00d-f116bea02a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "def main(search_config, gpus_per_trial=1):\n",
    "    scheduler = ASHAScheduler(\n",
    "        time_attr=\"training_iteration\",\n",
    "        max_t=search_config[\"max_num_epochs\"],\n",
    "        grace_period=1,\n",
    "        reduction_factor=2,\n",
    "    )\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(train_model),  # train_model accepts config\n",
    "            resources={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"val_loss\",          # optimize validation loss\n",
    "            mode=\"min\",\n",
    "            scheduler=scheduler,\n",
    "            num_samples=search_config[\"num_trials\"],\n",
    "        ),\n",
    "        param_space=search_config,\n",
    "    )\n",
    "\n",
    "    results = tuner.fit()\n",
    "    best_result = results.get_best_result(\"val_loss\", \"min\")\n",
    "\n",
    "    print(f\"\\n✅ Best trial config: {best_result.config}\")\n",
    "    print(f\"✅ Best trial final validation loss: {best_result.metrics['val_loss']:.4f}\")\n",
    "    print(f\"✅ Best trial final validation R²: {best_result.metrics['val_r2']:.4f}\")\n",
    "\n",
    "    test_best_model(best_result, smoke_test=search_config.get(\"smoke_test\", False))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a709fbac-3cf4-4d74-be22-344e7818de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_config = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"hidden_dim\": tune.choice([16, 32, 64]),\n",
    "    \"batch_size\": tune.choice([16, 32, 64]),\n",
    "    \"max_num_epochs\": 10,\n",
    "    \"num_trials\": 10,\n",
    "    \"smoke_test\": False,\n",
    "    \"log_tb\": True,  # keep if you use TensorBoard logging\n",
    "}\n",
    "\n",
    "r = main(search_config, gpus_per_trial=1 if torch.cuda.is_available() else 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17edc845-1387-4788-ba68-112418bf77ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f2b96-60b3-41ee-a016-04bc9990005a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0788e762-6360-48dc-a78a-6a503d62dc1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed739fd-1e5d-4a46-94b2-6e85a9fab006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebf11fc-7359-46b7-9335-3cdef6acf927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1ae894-85c7-4c12-b102-4236c9d7ead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from ray import tune\n",
    "from ray.air import session\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "# Step 1: Generate synthetic regression data\n",
    "def get_data():\n",
    "    np.random.seed(0)\n",
    "    X = np.random.rand(1000, 5)\n",
    "    y = 3 * X[:, 0] + 2 * X[:, 1] + np.random.randn(1000) * 0.1\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    return (\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).unsqueeze(1),\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "        torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "    )\n",
    "\n",
    "# Step 2: Define the model\n",
    "class SimpleRegressor0(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class SimpleRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Training helper\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Step 4: Validation helper\n",
    "def validate(model, X_val, y_val, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = model(X_val)\n",
    "        val_loss = criterion(val_pred, y_val).item()\n",
    "        val_r2 = 1 - val_loss / torch.var(y_val).item()  # pseudo R²\n",
    "    return val_loss, val_r2\n",
    "\n",
    "# Step 5: Training function for Ray Tune\n",
    "def train_model(config):\n",
    "    X_train, y_train, X_val, y_val = get_data()\n",
    "    model = SimpleRegressor(input_dim=5, hidden_dim=config[\"hidden_dim\"])\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(X_train, y_train),\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    for epoch in range(config[\"max_num_epochs\"]):\n",
    "        train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss, val_r2 = validate(model, X_val, y_val, criterion)\n",
    "        session.report({\"loss\": val_loss, \"accuracy\": val_r2})\n",
    "\n",
    "# Step 6: Test best model\n",
    "def test_best_model(best_result, smoke_test=False):\n",
    "    X_train, y_train, X_val, y_val = get_data()\n",
    "    model = SimpleRegressor(input_dim=5, hidden_dim=best_result.config[\"hidden_dim\"])\n",
    "    # No checkpoint loading here since checkpointing is off; retrain model on full data if needed.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(X_val)\n",
    "        mse = nn.MSELoss()(preds, y_val).item()\n",
    "        r2 = 1 - mse / torch.var(y_val).item()\n",
    "        print(f\"[TEST] Final MSE: {mse:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "# Step 7: Main function\n",
    "def main(config, gpus_per_trial=1):\n",
    "    scheduler = ASHAScheduler(\n",
    "        time_attr=\"training_iteration\",\n",
    "        max_t=config[\"max_num_epochs\"],\n",
    "        grace_period=1,\n",
    "        reduction_factor=2,\n",
    "    )\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(train_model),\n",
    "            resources={\"cpu\": 2, \"gpu\": gpus_per_trial}\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"loss\",\n",
    "            mode=\"min\",\n",
    "            scheduler=scheduler,\n",
    "            num_samples=config[\"num_trials\"],\n",
    "        ),\n",
    "        param_space=config,\n",
    "    )\n",
    "\n",
    "    results = tuner.fit()\n",
    "    best_result = results.get_best_result(\"loss\", \"min\")\n",
    "\n",
    "    print(f\"\\n✅ Best trial config: {best_result.config}\")\n",
    "    print(f\"✅ Best trial final validation loss: {best_result.metrics['loss']:.4f}\")\n",
    "    print(f\"✅ Best trial final validation R²: {best_result.metrics['accuracy']:.4f}\")\n",
    "\n",
    "    test_best_model(best_result, smoke_test=config.get(\"smoke_test\", False))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Step 8: Run tuning\n",
    "search_config = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"hidden_dim\": tune.choice([16, 32, 64]),\n",
    "    \"batch_size\": tune.choice([16, 32, 64]),\n",
    "    \"max_num_epochs\": 10,\n",
    "    \"num_trials\": 5, #10\n",
    "    \"smoke_test\": False,\n",
    "}\n",
    "\n",
    "r = main(search_config, gpus_per_trial=1 if torch.cuda.is_available() else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3628e146-0e79-4138-b078-6187045bac8e",
   "metadata": {},
   "source": [
    "So negative R² means your model fits worse than a naive baseline that predicts the average target.\n",
    "\n",
    "If you get negative R² consistently, it usually means:\n",
    "\n",
    "Your model is underfitting badly.\n",
    "\n",
    "Or your training process has a bug (bad labels, wrong predictions, data leakage, etc.).\n",
    "\n",
    "In practice, an R² near 1 is great, around 0 means “no better than mean,” and below 0 is poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23ec023-1b0a-42f1-a2c5-a4e8ef72bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc98183-677a-485f-81f8-4b849ce1aaec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dddf285-98b9-497c-9584-69e6b50a5ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580fc1e3-5bc1-4531-9bdb-7886bd07ec89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d295d45-94aa-438b-98d7-486497d8432d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d76f28b9-c483-48b6-a82e-698e78a3e1f0",
   "metadata": {},
   "source": [
    "# with train loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f90d502-db1d-4027-ba91-158f22786226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from ray import tune\n",
    "from ray.air import session\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    np.random.seed(0)\n",
    "    X = np.random.rand(1000, 5)\n",
    "    y = 3 * X[:, 0] + 2 * X[:, 1] + np.random.randn(1000) * 0.1\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    return (\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).unsqueeze(1),\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "        torch.tensor(y_val, dtype=torch.float32).unsqueeze(1),\n",
    "    )\n",
    "\n",
    "\n",
    "class SimpleRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    for xb, yb in train_loader:\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        count += xb.size(0)\n",
    "    return total_loss / count\n",
    "\n",
    "\n",
    "def r2_score(preds, targets):\n",
    "    ss_res = torch.sum((targets - preds) ** 2)\n",
    "    ss_tot = torch.sum((targets - torch.mean(targets)) ** 2)\n",
    "    return 1 - ss_res / ss_tot\n",
    "\n",
    "\n",
    "def validate(model, X_val, y_val, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = model(X_val)\n",
    "        val_loss = criterion(val_pred, y_val).item()\n",
    "        val_r2 = r2_score(val_pred, y_val).item()\n",
    "    return val_loss, val_r2\n",
    "\n",
    "\n",
    "def train_model(config):\n",
    "    X_train, y_train, X_val, y_val = get_data()\n",
    "    model = SimpleRegressor(input_dim=5, hidden_dim=config[\"hidden_dim\"])\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(X_train, y_train),\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    for epoch in range(config[\"max_num_epochs\"]):\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss, val_r2 = validate(model, X_val, y_val, criterion)\n",
    "\n",
    "        session.report(\n",
    "            {\n",
    "                \"train_loss\": train_loss,\n",
    "                \"loss\": val_loss,    # Tune optimizes this metric\n",
    "                \"val_r2\": val_r2,\n",
    "                \"epoch\": epoch,\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "def test_best_model(best_result, smoke_test=False):\n",
    "    X_train, y_train, X_val, y_val = get_data()\n",
    "    model = SimpleRegressor(input_dim=5, hidden_dim=best_result.config[\"hidden_dim\"])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(X_val)\n",
    "        mse = nn.MSELoss()(preds, y_val).item()\n",
    "        r2 = 1 - mse / torch.var(y_val).item()\n",
    "        print(f\"[TEST] Final MSE: {mse:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "\n",
    "def main(config, gpus_per_trial=1):\n",
    "    scheduler = ASHAScheduler(\n",
    "        time_attr=\"training_iteration\",\n",
    "        max_t=config[\"max_num_epochs\"],\n",
    "        grace_period=1,\n",
    "        reduction_factor=2,\n",
    "    )\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(train_model),\n",
    "            resources={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"loss\",\n",
    "            mode=\"min\",\n",
    "            scheduler=scheduler,\n",
    "            num_samples=config[\"num_trials\"],\n",
    "        ),\n",
    "        param_space=config,\n",
    "    )\n",
    "\n",
    "    results = tuner.fit()\n",
    "    best_result = results.get_best_result(\"loss\", \"min\")\n",
    "\n",
    "    print(f\"\\n✅ Best trial config: {best_result.config}\")\n",
    "    print(f\"✅ Best trial final validation loss: {best_result.metrics['loss']:.4f}\")\n",
    "    print(f\"✅ Best trial final validation R²: {best_result.metrics['val_r2']:.4f}\")\n",
    "\n",
    "    test_best_model(best_result, smoke_test=config.get(\"smoke_test\", False))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "search_config = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"hidden_dim\": tune.choice([16, 32, 64]),\n",
    "    \"batch_size\": tune.choice([16, 32, 64]),\n",
    "    \"max_num_epochs\": 10,\n",
    "    \"num_trials\": 5,\n",
    "    \"smoke_test\": False,\n",
    "}\n",
    "\n",
    "r = main(search_config, gpus_per_trial=1 if torch.cuda.is_available() else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3268be3c-f6e0-45dd-948f-16bc655956f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae1bcb4-9172-4021-a982-f06e68c6f11a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e412a5ee-3560-4fdc-b0b9-45db14a991f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec3f8c4-934b-4a9c-b7df-0ab78f02ee4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6556e68c-f052-4889-b21c-4fe3402aa9f8",
   "metadata": {},
   "source": [
    "### with checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f01373-1e5a-4849-82d7-b66e77741b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e97762b-866d-44f0-b502-fec38ba0fbd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8517d3-d500-4c34-b101-6e71a682e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade ray\n",
    "!pip install -U ray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00106cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.48.0\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "print(ray.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a66c2db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ray.air.checkpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mair\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcheckpoint\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(checkpoint)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ray.air.checkpoint'"
     ]
    }
   ],
   "source": [
    "import ray.air.checkpoint as checkpoint\n",
    "print(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5062c12a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cad97d7-7c3f-4c5f-b77e-95f4e0ddb082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-07-31 19:40:17</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:24.70        </td></tr>\n",
       "<tr><td>Memory:      </td><td>8.6/9.6 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=5<br>Bracket: Iter 8.000: -0.013298703357577324 | Iter 4.000: -0.019742880947887897 | Iter 2.000: -0.059881534427404404 | Iter 1.000: -0.15246325731277466<br>Logical resource usage: 2.0/4 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  hidden_dim</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">     loss</th><th style=\"text-align: right;\">   val_r2</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_4687b_00000</td><td>TERMINATED</td><td>200.40.20.243:479073</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">0.000135114</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.44844</td><td style=\"text-align: right;\">   7.55799  </td><td style=\"text-align: right;\">7.60163  </td><td style=\"text-align: right;\">-5.67088 </td></tr>\n",
       "<tr><td>train_model_4687b_00001</td><td>TERMINATED</td><td>200.40.20.243:479074</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">0.0189052  </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         2.76466</td><td style=\"text-align: right;\">   0.0119686</td><td style=\"text-align: right;\">0.0152264</td><td style=\"text-align: right;\"> 0.986638</td></tr>\n",
       "<tr><td>train_model_4687b_00002</td><td>TERMINATED</td><td>200.40.20.243:479227</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">0.000142155</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.91692</td><td style=\"text-align: right;\">   7.33057  </td><td style=\"text-align: right;\">7.38299  </td><td style=\"text-align: right;\">-5.47901 </td></tr>\n",
       "<tr><td>train_model_4687b_00003</td><td>TERMINATED</td><td>200.40.20.243:479226</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">0.00448687 </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         2.00379</td><td style=\"text-align: right;\">   0.101661 </td><td style=\"text-align: right;\">0.0786322</td><td style=\"text-align: right;\"> 0.930996</td></tr>\n",
       "<tr><td>train_model_4687b_00004</td><td>TERMINATED</td><td>200.40.20.243:479343</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">0.022203   </td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         1.4622 </td><td style=\"text-align: right;\">   0.0179951</td><td style=\"text-align: right;\">0.0144127</td><td style=\"text-align: right;\"> 0.987352</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=479073)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/marcos/ray_results/train_model_2025-07-31_19-39-53/train_model_4687b_00000_0_batch_size=16,hidden_dim=16,lr=0.0001_2025-07-31_19-39-53/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=479074)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/marcos/ray_results/train_model_2025-07-31_19-39-53/train_model_4687b_00001_1_batch_size=32,hidden_dim=64,lr=0.0189_2025-07-31_19-39-53/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=479074)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/marcos/ray_results/train_model_2025-07-31_19-39-53/train_model_4687b_00001_1_batch_size=32,hidden_dim=64,lr=0.0189_2025-07-31_19-39-53/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=479074)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/marcos/ray_results/train_model_2025-07-31_19-39-53/train_model_4687b_00001_1_batch_size=32,hidden_dim=64,lr=0.0189_2025-07-31_19-39-53/checkpoint_000002)\n",
      "\u001b[36m(train_model pid=479074)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/marcos/ray_results/train_model_2025-07-31_19-39-53/train_model_4687b_00001_1_batch_size=32,hidden_dim=64,lr=0.0189_2025-07-31_19-39-53/checkpoint_000003)\n",
      "\u001b[36m(train_model pid=479074)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/marcos/ray_results/train_model_2025-07-31_19-39-53/train_model_4687b_00001_1_batch_size=32,hidden_dim=64,lr=0.0189_2025-07-31_19-39-53/checkpoint_000004)\n",
      "\u001b[36m(train_model pid=479074)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/marcos/ray_results/train_model_2025-07-31_19-39-53/train_model_4687b_00001_1_batch_size=32,hidden_dim=64,lr=0.0189_2025-07-31_19-39-53/checkpoint_000005)\n",
      "\u001b[36m(train_model pid=479074)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/marcos/ray_results/train_model_2025-07-31_19-39-53/train_model_4687b_00001_1_batch_size=32,hidden_dim=64,lr=0.0189_2025-07-31_19-39-53/checkpoint_000006)\n",
      "\u001b[36m(train_model pid=479074)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/marcos/ray_results/train_model_2025-07-31_19-39-53/train_model_4687b_00001_1_batch_size=32,hidden_dim=64,lr=0.0189_2025-07-31_19-39-53/checkpoint_000007)\n",
      "\u001b[36m(train_model pid=479074)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/marcos/ray_results/train_model_2025-07-31_19-39-53/train_model_4687b_00001_1_batch_size=32,hidden_dim=64,lr=0.0189_2025-07-31_19-39-53/checkpoint_000008)\n",
      "\u001b[36m(train_model pid=479074)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/marcos/ray_results/train_model_2025-07-31_19-39-53/train_model_4687b_00001_1_batch_size=32,hidden_dim=64,lr=0.0189_2025-07-31_19-39-53/checkpoint_000009)\n",
      "\u001b[36m(train_model pid=479227)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/marcos/ray_results/train_model_2025-07-31_19-39-53/train_model_4687b_00002_2_batch_size=32,hidden_dim=16,lr=0.0001_2025-07-31_19-39-53/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=479226)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/marcos/ray_results/train_model_2025-07-31_19-39-53/train_model_4687b_00003_3_batch_size=16,hidden_dim=32,lr=0.0045_2025-07-31_19-39-53/checkpoint_000000)\n",
      "\u001b[36m(train_model pid=479226)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/marcos/ray_results/train_model_2025-07-31_19-39-53/train_model_4687b_00003_3_batch_size=16,hidden_dim=32,lr=0.0045_2025-07-31_19-39-53/checkpoint_000001)\n",
      "\u001b[36m(train_model pid=479343)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/marcos/ray_results/train_model_2025-07-31_19-39-53/train_model_4687b_00004_4_batch_size=16,hidden_dim=16,lr=0.0222_2025-07-31_19-39-53/checkpoint_000001)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-07-31 19:40:17,981\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/marcos/ray_results/train_model_2025-07-31_19-39-53' in 0.0059s.\n",
      "2025-07-31 19:40:17,989\tINFO tune.py:1041 -- Total run time: 24.73 seconds (24.70 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Best trial config: {'lr': 0.022203016839658106, 'hidden_dim': 16, 'batch_size': 16, 'max_num_epochs': 10, 'num_trials': 5, 'smoke_test': False}\n",
      "✅ Best trial final validation loss: 0.0144\n",
      "✅ Best trial final validation R²: 0.9874\n",
      "[TEST] Final MSE: 0.0144, R²: 0.9874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_477962/1151875944.py:136: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f\"{checkpoint_path}/model.pth\")\n",
      "\u001b[36m(train_model pid=479343)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/marcos/ray_results/train_model_2025-07-31_19-39-53/train_model_4687b_00004_4_batch_size=16,hidden_dim=16,lr=0.0222_2025-07-31_19-39-53/checkpoint_000007)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from ray.air import session\n",
    "\n",
    "\n",
    "\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "\n",
    "#from ray.air.checkpoint import Checkpoint\n",
    "# 2 -okfrom ray.train import Checkpoint\n",
    "from ray.tune import Checkpoint\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import tempfile\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    np.random.seed(0)\n",
    "    X = np.random.rand(1000, 5)\n",
    "    y = 3 * X[:, 0] + 2 * X[:, 1] + np.random.randn(1000) * 0.1\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    return (\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).unsqueeze(1),\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "        torch.tensor(y_val, dtype=torch.float32).unsqueeze(1),\n",
    "    )\n",
    "\n",
    "\n",
    "class SimpleRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    for xb, yb in train_loader:\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        count += xb.size(0)\n",
    "    return total_loss / count\n",
    "\n",
    "\n",
    "def r2_score(preds, targets):\n",
    "    ss_res = torch.sum((targets - preds) ** 2)\n",
    "    ss_tot = torch.sum((targets - torch.mean(targets)) ** 2)\n",
    "    return 1 - ss_res / ss_tot\n",
    "\n",
    "\n",
    "def validate(model, X_val, y_val, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = model(X_val)\n",
    "        val_loss = criterion(val_pred, y_val).item()\n",
    "        val_r2 = r2_score(val_pred, y_val).item()\n",
    "    return val_loss, val_r2\n",
    "\n",
    "\n",
    "def train_model(config):\n",
    "    X_train, y_train, X_val, y_val = get_data()\n",
    "    model = SimpleRegressor(input_dim=5, hidden_dim=config[\"hidden_dim\"])\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(X_train, y_train),\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    for epoch in range(config[\"max_num_epochs\"]):\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss, val_r2 = validate(model, X_val, y_val, criterion)\n",
    "\n",
    "        # Save checkpoint to temp directory\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            path = f\"{tmpdir}/model.pth\"\n",
    "            torch.save(model.state_dict(), path)\n",
    "            ckpt = Checkpoint.from_directory(tmpdir)\n",
    "\n",
    "            #session.report(\n",
    "            tune.report(\n",
    "                {\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"loss\": val_loss,\n",
    "                    \"val_r2\": val_r2,\n",
    "                    \"epoch\": epoch,\n",
    "                },\n",
    "                checkpoint=ckpt,\n",
    "            )\n",
    "\n",
    "\n",
    "def test_best_model(best_result, smoke_test=False):\n",
    "    X_train, y_train, X_val, y_val = get_data()\n",
    "    model = SimpleRegressor(input_dim=5, hidden_dim=best_result.config[\"hidden_dim\"])\n",
    "\n",
    "    # Load checkpoint weights\n",
    "    checkpoint_path = best_result.checkpoint.to_directory()\n",
    "    state_dict = torch.load(f\"{checkpoint_path}/model.pth\")\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(X_val)\n",
    "        mse = nn.MSELoss()(preds, y_val).item()\n",
    "        r2 = 1 - mse / torch.var(y_val).item()\n",
    "        print(f\"[TEST] Final MSE: {mse:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "\n",
    "def main(config, gpus_per_trial=1):\n",
    "    scheduler = ASHAScheduler(\n",
    "        time_attr=\"training_iteration\",\n",
    "        max_t=config[\"max_num_epochs\"],\n",
    "        grace_period=1,\n",
    "        reduction_factor=2,\n",
    "    )\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(train_model),\n",
    "            resources={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"loss\",\n",
    "            mode=\"min\",\n",
    "            scheduler=scheduler,\n",
    "            num_samples=config[\"num_trials\"],\n",
    "        ),\n",
    "        param_space=config,\n",
    "    )\n",
    "\n",
    "    results = tuner.fit()\n",
    "    best_result = results.get_best_result(\"loss\", \"min\")\n",
    "\n",
    "    print(f\"\\n✅ Best trial config: {best_result.config}\")\n",
    "    print(f\"✅ Best trial final validation loss: {best_result.metrics['loss']:.4f}\")\n",
    "    print(f\"✅ Best trial final validation R²: {best_result.metrics['val_r2']:.4f}\")\n",
    "\n",
    "    test_best_model(best_result, smoke_test=config.get(\"smoke_test\", False))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "search_config = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"hidden_dim\": tune.choice([16, 32, 64]),\n",
    "    \"batch_size\": tune.choice([16, 32, 64]),\n",
    "    \"max_num_epochs\": 10,\n",
    "    \"num_trials\": 5,\n",
    "    \"smoke_test\": False,\n",
    "}\n",
    "\n",
    "r = main(search_config, gpus_per_trial=1 if torch.cuda.is_available() else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e19893c-e2d5-4d66-a4fe-e3764d69f67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResultGrid<[\n",
       "  Result(\n",
       "    metrics={'train_loss': 7.557986307144165, 'loss': 7.601632118225098, 'val_r2': -5.6708807945251465, 'epoch': 0},\n",
       "    path='/home/marcos/ray_results/train_model_2025-07-31_19-39-53/train_model_4687b_00000_0_batch_size=16,hidden_dim=16,lr=0.0001_2025-07-31_19-39-53',\n",
       "    filesystem='local',\n",
       "    checkpoint=Checkpoint(filesystem=local, path=/home/marcos/ray_results/train_model_2025-07-31_19-39-53/train_model_4687b_00000_0_batch_size=16,hidden_dim=16,lr=0.0001_2025-07-31_19-39-53/checkpoint_000000)\n",
       "  ),\n",
       "  Result(\n",
       "    metrics={'train_loss': 0.011968582347035408, 'loss': 0.01522635668516159, 'val_r2': 0.9866379499435425, 'epoch': 9},\n",
       "    path='/home/marcos/ray_results/train_model_2025-07-31_19-39-53/train_model_4687b_00001_1_batch_size=32,hidden_dim=64,lr=0.0189_2025-07-31_19-39-53',\n",
       "    filesystem='local',\n",
       "    checkpoint=Checkpoint(filesystem=local, path=/home/marcos/ray_results/train_model_2025-07-31_19-39-53/train_model_4687b_00001_1_batch_size=32,hidden_dim=64,lr=0.0189_2025-07-31_19-39-53/checkpoint_000009)\n",
       "  ),\n",
       "  Result(\n",
       "    metrics={'train_loss': 7.330566902160644, 'loss': 7.382990837097168, 'val_r2': -5.479010105133057, 'epoch': 0},\n",
       "    path='/home/marcos/ray_results/train_model_2025-07-31_19-39-53/train_model_4687b_00002_2_batch_size=32,hidden_dim=16,lr=0.0001_2025-07-31_19-39-53',\n",
       "    filesystem='local',\n",
       "    checkpoint=Checkpoint(filesystem=local, path=/home/marcos/ray_results/train_model_2025-07-31_19-39-53/train_model_4687b_00002_2_batch_size=32,hidden_dim=16,lr=0.0001_2025-07-31_19-39-53/checkpoint_000000)\n",
       "  ),\n",
       "  Result(\n",
       "    metrics={'train_loss': 0.10166141711175442, 'loss': 0.0786321833729744, 'val_r2': 0.9309956431388855, 'epoch': 1},\n",
       "    path='/home/marcos/ray_results/train_model_2025-07-31_19-39-53/train_model_4687b_00003_3_batch_size=16,hidden_dim=32,lr=0.0045_2025-07-31_19-39-53',\n",
       "    filesystem='local',\n",
       "    checkpoint=Checkpoint(filesystem=local, path=/home/marcos/ray_results/train_model_2025-07-31_19-39-53/train_model_4687b_00003_3_batch_size=16,hidden_dim=32,lr=0.0045_2025-07-31_19-39-53/checkpoint_000001)\n",
       "  ),\n",
       "  Result(\n",
       "    metrics={'train_loss': 0.017995138596743347, 'loss': 0.014412740245461464, 'val_r2': 0.9873519539833069, 'epoch': 7},\n",
       "    path='/home/marcos/ray_results/train_model_2025-07-31_19-39-53/train_model_4687b_00004_4_batch_size=16,hidden_dim=16,lr=0.0222_2025-07-31_19-39-53',\n",
       "    filesystem='local',\n",
       "    checkpoint=Checkpoint(filesystem=local, path=/home/marcos/ray_results/train_model_2025-07-31_19-39-53/train_model_4687b_00004_4_batch_size=16,hidden_dim=16,lr=0.0222_2025-07-31_19-39-53/checkpoint_000007)\n",
       "  )\n",
       "]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb2a607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
